{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPojRlNQPJub"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOtnY2y8Om-1"
   },
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:29:15.946250Z",
     "iopub.status.busy": "2022-05-24T04:29:15.945916Z",
     "iopub.status.idle": "2022-05-24T04:29:15.954129Z",
     "shell.execute_reply": "2022-05-24T04:29:15.950838Z",
     "shell.execute_reply.started": "2022-05-24T04:29:15.946218Z"
    },
    "id": "1Mf4kbp8yOxd"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "initialization failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;31mSystemError\u001b[0m: <built-in method __contains__ of dict object at 0x7fddc9b5ae80> returned a result with an error set",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-afe8861271fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmonitoring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/client/pywrap_tf_session.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_TF_SetTarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pywrap_tf_session\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_TF_SetConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: initialization failed"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import Dict, List, Optional, Text, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVb2hhOcgVwU"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awWJ00JeOyuO"
   },
   "source": [
    "Enter the Unix glob file pattern of the data files.\n",
    "\n",
    "Here we load the training data. All the data are stored in TensorFlow Record files.\n",
    "Replace 'train' with 'eval' or 'test' to load the evaluation or testing data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:29:17.148452Z",
     "iopub.status.busy": "2022-05-24T04:29:17.148110Z",
     "iopub.status.idle": "2022-05-24T04:29:17.153940Z",
     "shell.execute_reply": "2022-05-24T04:29:17.152900Z",
     "shell.execute_reply.started": "2022-05-24T04:29:17.148396Z"
    },
    "id": "fhHMUbBoOg0k"
   },
   "outputs": [],
   "source": [
    "file_pattern = '/kaggle/input/next-day-wildfire-spread/next_day_wildfire_spread_train*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OabUJMGqO9NE"
   },
   "source": [
    "Run the following three cells to define the required library functions for loading the data.\n",
    "\n",
    "The first cell defines the name of the variables in the input files and the corrresponding data statistics. The statistics can be used for preprocessing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:29:18.153101Z",
     "iopub.status.busy": "2022-05-24T04:29:18.152780Z",
     "iopub.status.idle": "2022-05-24T04:29:18.165788Z",
     "shell.execute_reply": "2022-05-24T04:29:18.164694Z",
     "shell.execute_reply.started": "2022-05-24T04:29:18.153067Z"
    },
    "id": "GTTV3tjjCcdn"
   },
   "outputs": [],
   "source": [
    "\"\"\"Constants for the data reader.\"\"\"\n",
    "\n",
    "INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph', \n",
    "                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']\n",
    "\n",
    "OUTPUT_FEATURES = ['FireMask', ]\n",
    "\n",
    "# Data statistics \n",
    "# For each variable, the statistics are ordered in the form:\n",
    "# (min_clip, max_clip, mean, standard deviation)\n",
    "DATA_STATS = {\n",
    "    # Elevation in m.\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'elevation': (0.0, 3141.0, 657.3003, 649.0147),\n",
    "    \n",
    "    # Drought Index (Palmer Drought Severity Index)\n",
    "    # 0.1 percentile, 99.9 percentile\n",
    "    'pdsi': (-6.12974870967865, 7.876040384292651, -0.0052714925, 2.6823447),\n",
    "    \n",
    "    #Vegetation index (times 10,000 maybe, since it's supposed to be b/w -1 and 1?)\n",
    "    'NDVI': (-9821.0, 9996.0, 5157.625, 2466.6677),  # min, max\n",
    "   \n",
    "    # Precipitation in mm.\n",
    "    # Negative values do not make sense, so min is set to 0.\n",
    "    # 0., 99.9 percentile\n",
    "    'pr': (0.0, 44.53038024902344, 1.7398051, 4.482833),\n",
    "   \n",
    "    # Specific humidity.\n",
    "    # Negative values do not make sense, so min is set to 0.\n",
    "    # The range of specific humidity is up to 100% so max is 1.\n",
    "    'sph': (0., 1., 0.0071658953, 0.0042835088),\n",
    "    \n",
    "    # Wind direction in degrees clockwise from north.\n",
    "    # Thus min set to 0 and max set to 360.\n",
    "    'th': (0., 360.0, 190.32976, 72.59854),\n",
    "    \n",
    "    # Min/max temperature in Kelvin.\n",
    "    \n",
    "    #Min temp\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmn': (253.15, 298.94891357421875, 281.08768, 8.982386),\n",
    "    \n",
    "    #Max temp\n",
    "    # -20 degree C, 99.9 percentile\n",
    "    'tmmx': (253.15, 315.09228515625, 295.17383, 9.815496),\n",
    "    \n",
    "    # Wind speed in m/s.\n",
    "    # Negative values do not make sense, given there is a wind direction.\n",
    "    # 0., 99.9 percentile\n",
    "    'vs': (0.0, 10.024310074806237, 3.8500874, 1.4109988),\n",
    "    \n",
    "    # NFDRS fire danger index energy release component expressed in BTU's per\n",
    "    # square foot.\n",
    "    # Negative values do not make sense. Thus min set to zero.\n",
    "    # 0., 99.9 percentile\n",
    "    'erc': (0.0, 106.24891662597656, 37.326267, 20.846027),\n",
    "    \n",
    "    # Population density\n",
    "    # min, 99.9 percentile\n",
    "    'population': (0., 2534.06298828125, 25.531384, 154.72331),\n",
    "    \n",
    "    # We don't want to normalize the FireMasks.\n",
    "    # 1 indicates fire, 0 no fire, -1 unlabeled data\n",
    "    'PrevFireMask': (-1., 1., 0., 1.),\n",
    "    'FireMask': (-1., 1., 0., 1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines cropping functions for extracting regions of the desired size from the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:29:19.434570Z",
     "iopub.status.busy": "2022-05-24T04:29:19.434173Z",
     "iopub.status.idle": "2022-05-24T04:29:19.446286Z",
     "shell.execute_reply": "2022-05-24T04:29:19.445243Z",
     "shell.execute_reply.started": "2022-05-24T04:29:19.434503Z"
    },
    "id": "QqGYv21hD-2q"
   },
   "outputs": [],
   "source": [
    "\"\"\"Library of common functions used in deep learning neural networks.\n",
    "\"\"\"\n",
    "#YOU PROBABLY WILL NOT USE THESE.\n",
    "\n",
    "def random_crop_input_and_output_images(\n",
    "    input_img: tf.Tensor,\n",
    "    output_img: tf.Tensor,\n",
    "    sample_size: int,\n",
    "    num_in_channels: int,\n",
    "    num_out_channels: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Randomly axis-align crop input and output image tensors.\n",
    "\n",
    "  Args:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "    sample_size: side length (square) to crop to.\n",
    "    num_in_channels: number of channels in input_img.\n",
    "    num_out_channels: number of channels in output_img.\n",
    "  Returns:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "  \"\"\"\n",
    "  combined = tf.concat([input_img, output_img], axis=2)\n",
    "  combined = tf.image.random_crop(\n",
    "      combined,\n",
    "      [sample_size, sample_size, num_in_channels + num_out_channels])\n",
    "  input_img = combined[:, :, 0:num_in_channels]\n",
    "  output_img = combined[:, :, -num_out_channels:]\n",
    "  return input_img, output_img\n",
    "\n",
    "\n",
    "def center_crop_input_and_output_images(\n",
    "    input_img: tf.Tensor,\n",
    "    output_img: tf.Tensor,\n",
    "    sample_size: int,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Center crops input and output image tensors.\n",
    "\n",
    "  Args:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "    sample_size: side length (square) to crop to.\n",
    "  Returns:\n",
    "    input_img: tensor with dimensions HWC.\n",
    "    output_img: tensor with dimensions HWC.\n",
    "  \"\"\"\n",
    "  central_fraction = sample_size / input_img.shape[0]\n",
    "  input_img = tf.image.central_crop(input_img, central_fraction)\n",
    "  output_img = tf.image.central_crop(output_img, central_fraction)\n",
    "  return input_img, output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell provides code for parsing the contents of the TensorFlow Record files. In addition to loading the data, it also offers functions for various preprocessing operations, such as clipping, rescaling, or normalizing the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:29:21.193625Z",
     "iopub.status.busy": "2022-05-24T04:29:21.193117Z",
     "iopub.status.idle": "2022-05-24T04:29:21.223954Z",
     "shell.execute_reply": "2022-05-24T04:29:21.223062Z",
     "shell.execute_reply.started": "2022-05-24T04:29:21.193584Z"
    },
    "id": "VBvI9FuGEC09"
   },
   "outputs": [],
   "source": [
    "\"\"\"Dataset reader for Earth Engine data.\"\"\"\n",
    "\n",
    "def _get_base_key(key: Text) -> Text:\n",
    "  \"\"\"Extracts the base key from the provided key.\n",
    "\n",
    "  Earth Engine exports TFRecords containing each data variable with its\n",
    "  corresponding variable name. In the case of time sequences, the name of the\n",
    "  data variable is of the form 'variable_1', 'variable_2', ..., 'variable_n',\n",
    "  where 'variable' is the name of the variable, and n the number of elements\n",
    "  in the time sequence. Extracting the base key ensures that each step of the\n",
    "  time sequence goes through the same normalization steps.\n",
    "  The base key obeys the following naming pattern: '([a-zA-Z]+)'\n",
    "  For instance, for an input key 'variable_1', this function returns 'variable'.\n",
    "  For an input key 'variable', this function simply returns 'variable'.\n",
    "\n",
    "  Args:\n",
    "    key: Input key.\n",
    "\n",
    "  Returns:\n",
    "    The corresponding base key.\n",
    "\n",
    "  Raises:\n",
    "    ValueError when `key` does not match the expected pattern.\n",
    "  \"\"\"\n",
    "  match = re.match(r'([a-zA-Z]+)', key)\n",
    "  if match:\n",
    "    return match.group(1)\n",
    "  raise ValueError(\n",
    "      'The provided key does not match the expected pattern: {}'.format(key))\n",
    "\n",
    "\n",
    "def _clip_and_rescale(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "  \"\"\"Clips and rescales inputs with the stats corresponding to `key`.\n",
    "\n",
    "  Args:\n",
    "    inputs: Inputs to clip and rescale.\n",
    "    key: Key describing the inputs.\n",
    "\n",
    "  Returns:\n",
    "    Clipped and rescaled input.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if there are no data statistics available for `key`.\n",
    "  \"\"\"\n",
    "  base_key = _get_base_key(key)\n",
    "  if base_key not in DATA_STATS:\n",
    "    raise ValueError(\n",
    "        'No data statistics available for the requested key: {}.'.format(key))\n",
    "  min_val, max_val, _, _ = DATA_STATS[base_key]\n",
    "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "  return tf.math.divide_no_nan((inputs - min_val), (max_val - min_val))\n",
    "\n",
    "\n",
    "def _clip_and_normalize(inputs: tf.Tensor, key: Text) -> tf.Tensor:\n",
    "  \"\"\"Clips and normalizes inputs with the stats corresponding to `key`.\n",
    "\n",
    "  Args:\n",
    "    inputs: Inputs to clip and normalize.\n",
    "    key: Key describing the inputs.\n",
    "\n",
    "  Returns:\n",
    "    Clipped and normalized input.\n",
    "\n",
    "  Raises:\n",
    "    ValueError if there are no data statistics available for `key`.\n",
    "  \"\"\"\n",
    "  base_key = _get_base_key(key)\n",
    "  if base_key not in DATA_STATS:\n",
    "    raise ValueError(\n",
    "        'No data statistics available for the requested key: {}.'.format(key))\n",
    "  min_val, max_val, mean, std = DATA_STATS[base_key]\n",
    "  inputs = tf.clip_by_value(inputs, min_val, max_val)\n",
    "  inputs = inputs - mean\n",
    "  return tf.math.divide_no_nan(inputs, std)\n",
    "\n",
    "def _get_features_dict(\n",
    "    sample_size: int,\n",
    "    features: List[Text],\n",
    ") -> Dict[Text, tf.io.FixedLenFeature]:\n",
    "  \"\"\"Creates a features dictionary for TensorFlow IO.\n",
    "\n",
    "  Args:\n",
    "    sample_size: Size of the input tiles (square).\n",
    "    features: List of feature names.\n",
    "\n",
    "  Returns:\n",
    "    A features dictionary for TensorFlow IO.\n",
    "  \"\"\"\n",
    "  sample_shape = [sample_size, sample_size]\n",
    "  features = set(features)\n",
    "  columns = [\n",
    "      tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)\n",
    "      for _ in features\n",
    "  ]\n",
    "  return dict(zip(features, columns))\n",
    "\n",
    "\n",
    "def _parse_fn(\n",
    "    example_proto: tf.train.Example, data_size: int, sample_size: int,\n",
    "    num_in_channels: int, clip_and_normalize: bool,\n",
    "    clip_and_rescale: bool, random_crop: bool, center_crop: bool,\n",
    ") -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "  \"\"\"Reads a serialized example.\n",
    "\n",
    "  Args:\n",
    "    example_proto: A TensorFlow example protobuf.\n",
    "    data_size: Size of tiles (square) as read from input files.\n",
    "    sample_size: Size the tiles (square) when input into the model.\n",
    "    num_in_channels: Number of input channels.\n",
    "    clip_and_normalize: True if the data should be clipped and normalized.\n",
    "    clip_and_rescale: True if the data should be clipped and rescaled.\n",
    "    random_crop: True if the data should be randomly cropped.\n",
    "    center_crop: True if the data should be cropped in the center.\n",
    "\n",
    "  Returns:\n",
    "    (input_img, output_img) tuple of inputs and outputs to the ML model.\n",
    "  \"\"\"\n",
    "  if (random_crop and center_crop):\n",
    "    raise ValueError('Cannot have both random_crop and center_crop be True')\n",
    "  input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES\n",
    "  feature_names = input_features + output_features\n",
    "  features_dict = _get_features_dict(data_size, feature_names)\n",
    "  features = tf.io.parse_single_example(example_proto, features_dict)\n",
    "\n",
    "  if clip_and_normalize:\n",
    "    inputs_list = [\n",
    "        _clip_and_normalize(features.get(key), key) for key in input_features\n",
    "    ]\n",
    "  elif clip_and_rescale:\n",
    "    inputs_list = [\n",
    "        _clip_and_rescale(features.get(key), key) for key in input_features\n",
    "    ]\n",
    "  else:\n",
    "    inputs_list = [features.get(key) for key in input_features]\n",
    "  \n",
    "  inputs_stacked = tf.stack(inputs_list, axis=0)\n",
    "  input_img = tf.transpose(inputs_stacked, [1, 2, 0])\n",
    "\n",
    "  outputs_list = [features.get(key) for key in output_features]\n",
    "  assert outputs_list, 'outputs_list should not be empty'\n",
    "  outputs_stacked = tf.stack(outputs_list, axis=0)\n",
    "\n",
    "  outputs_stacked_shape = outputs_stacked.get_shape().as_list()\n",
    "  assert len(outputs_stacked.shape) == 3, ('outputs_stacked should be rank 3'\n",
    "                                            'but dimensions of outputs_stacked'\n",
    "                                            f' are {outputs_stacked_shape}')\n",
    "  output_img = tf.transpose(outputs_stacked, [1, 2, 0])\n",
    "\n",
    "  if random_crop:\n",
    "    input_img, output_img = random_crop_input_and_output_images(\n",
    "        input_img, output_img, sample_size, num_in_channels, 1)\n",
    "  if center_crop:\n",
    "    input_img, output_img = center_crop_input_and_output_images(\n",
    "        input_img, output_img, sample_size)\n",
    "  return input_img, output_img\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern: Text, data_size: int, sample_size: int,\n",
    "                batch_size: int, num_in_channels: int, compression_type: Text,\n",
    "                clip_and_normalize: bool, clip_and_rescale: bool,\n",
    "                random_crop: bool, center_crop: bool) -> tf.data.Dataset:\n",
    "  \"\"\"Gets the dataset from the file pattern.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: Input file pattern.\n",
    "    data_size: Size of tiles (square) as read from input files.\n",
    "    sample_size: Size the tiles (square) when input into the model.\n",
    "    batch_size: Batch size.\n",
    "    num_in_channels: Number of input channels.\n",
    "    compression_type: Type of compression used for the input files.\n",
    "    clip_and_normalize: True if the data should be clipped and normalized, False\n",
    "      otherwise.\n",
    "    clip_and_rescale: True if the data should be clipped and rescaled, False\n",
    "      otherwise.\n",
    "    random_crop: True if the data should be randomly cropped.\n",
    "    center_crop: True if the data shoulde be cropped in the center.\n",
    "\n",
    "  Returns:\n",
    "    A TensorFlow dataset loaded from the input file pattern, with features\n",
    "    described in the constants, and with the shapes determined from the input\n",
    "    parameters to this function.\n",
    "  \"\"\"\n",
    "  if (clip_and_normalize and clip_and_rescale):\n",
    "    raise ValueError('Cannot have both normalize and rescale.')\n",
    "  dataset = tf.data.Dataset.list_files(file_pattern)\n",
    "  dataset = dataset.interleave(\n",
    "      lambda x: tf.data.TFRecordDataset(x, compression_type=compression_type),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.map(\n",
    "      lambda x: _parse_fn(  # pylint: disable=g-long-lambda\n",
    "          x, data_size, sample_size, num_in_channels, clip_and_normalize,\n",
    "          clip_and_rescale, random_crop, center_crop),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:25:18.026843Z",
     "iopub.status.busy": "2022-05-24T04:25:18.026213Z",
     "iopub.status.idle": "2022-05-24T04:25:18.103899Z",
     "shell.execute_reply": "2022-05-24T04:25:18.102947Z",
     "shell.execute_reply.started": "2022-05-24T04:25:18.026789Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = np.array(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:25:32.870745Z",
     "iopub.status.busy": "2022-05-24T04:25:32.869743Z",
     "iopub.status.idle": "2022-05-24T04:25:32.877224Z",
     "shell.execute_reply": "2022-05-24T04:25:32.876267Z",
     "shell.execute_reply.started": "2022-05-24T04:25:32.870674Z"
    }
   },
   "outputs": [],
   "source": [
    "ds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdNDytnsPTVK"
   },
   "source": [
    "Load the dataset.\n",
    "\n",
    "The data are stored as 64x64 km regions. For each data sample, we extract a random 32x32 km region. In the following function call, we do not clip, rescale or normalize the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:33:08.926344Z",
     "iopub.status.busy": "2022-05-24T04:33:08.925963Z",
     "iopub.status.idle": "2022-05-24T04:33:09.002624Z",
     "shell.execute_reply": "2022-05-24T04:33:09.001484Z",
     "shell.execute_reply.started": "2022-05-24T04:33:08.926309Z"
    },
    "id": "X1jBBEinQbM0"
   },
   "outputs": [],
   "source": [
    "side_length = 32 #length of the side of the square you select (so, e.g. pick 64 if you don't want any random cropping)\n",
    "num_obs = 14979 #batch size\n",
    "\n",
    "dataset = get_dataset(\n",
    "      file_pattern,\n",
    "      data_size=64,\n",
    "      sample_size=side_length,\n",
    "      batch_size=num_obs,\n",
    "      num_in_channels=12,\n",
    "      compression_type=None,\n",
    "      clip_and_normalize=False,\n",
    "      clip_and_rescale=False,\n",
    "      random_crop=True,\n",
    "      center_crop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca8USco0PdG3"
   },
   "source": [
    "TF Datasets are loaded lazily, so materialize the first batch of inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:33:09.981712Z",
     "iopub.status.busy": "2022-05-24T04:33:09.981365Z",
     "iopub.status.idle": "2022-05-24T04:33:13.417142Z",
     "shell.execute_reply": "2022-05-24T04:33:13.416165Z",
     "shell.execute_reply.started": "2022-05-24T04:33:09.981679Z"
    },
    "id": "Ml7Rg8aCQiTT"
   },
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(dataset))\n",
    "#Are there two assignments happening on every iteration because dataset stores inputs with labels?\n",
    "print(inputs.shape) #(100, 32, 32, 12)\n",
    "#print(labels.shape) #(100, 32, 32, 1)\n",
    "#print(inputs[0, :, :, 11]) #Trying to grab the previous fire mask. (Apparent) success!\n",
    "#print(labels[0,:, :, 0]) #Ok, I think the labels are the fire mask. (That also accords with standard usage of the term.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to consider the average neighbor fire scores eventually. The function below helps me do that. Form: array_out = avg_neighbors(array_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:33:16.273409Z",
     "iopub.status.busy": "2022-05-24T04:33:16.272420Z",
     "iopub.status.idle": "2022-05-24T04:33:16.295256Z",
     "shell.execute_reply": "2022-05-24T04:33:16.294296Z",
     "shell.execute_reply.started": "2022-05-24T04:33:16.273339Z"
    }
   },
   "outputs": [],
   "source": [
    "#Eventually would like a function that takes in an input array of dimensions nxn, \n",
    "#outputs an array that gives avg of each cell's neighbors:\n",
    "def avg_neighbors(array_in):\n",
    "    #Check input\n",
    "    if array_in.shape[0] != array_in.shape[1]:\n",
    "        raise Exception('Only square arrays make sense here, since you\\'re analyzing square arrays.')\n",
    "    #Maybe should also do type-checking, but leave it for now.\n",
    "    \n",
    "    #Prepare the output array:\n",
    "    n = array_in.shape[0] \n",
    "    array_out = np.zeros((n,n))\n",
    "    \n",
    "    #Guess who doesn't know how to do signal processing in Python...\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == 0:\n",
    "                #Upper edge\n",
    "                if j == 0:\n",
    "                    #Upper left corner\n",
    "                    sum_neighbors = array_in[i+1, j] + array_in[i, j+1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                elif j == (n-1):\n",
    "                    #Upper right corner\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i+1, j] + array_in[i+1, j-1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                else:\n",
    "                    #Upper edge except corners\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i+1, j] + array_in[i, j+1] + array_in[i+1, j-1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/5\n",
    "                   \n",
    "            elif i == (n-1):\n",
    "                #Lower edge\n",
    "                if j == 0:\n",
    "                    #Lower left corner\n",
    "                    sum_neighbors = array_in[i-1, j] + array_in[i, j+1] + array_in[i-1, j+1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                elif j == (n-1):\n",
    "                    #Lower right corner\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i-1, j] + array_in[i-1, j-1]\n",
    "                    avg = sum_neighbors/3\n",
    "                    \n",
    "                else:\n",
    "                    #Lower edge except corners\n",
    "                    sum_neighbors = array_in[i, j-1] + array_in[i, j+1] + array_in[i-1, j] + array_in[i-1, j-1] + array_in[i-1, j+1]\n",
    "                    avg = sum_neighbors/5\n",
    "                    \n",
    "            else:\n",
    "                if j == 0:\n",
    "                    #Left edge except corners\n",
    "                    sum_neighbors = array_in[i-1, j] + array_in[i+1, j] + array_in[i, j+1] + array_in[i-1, j+1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/5\n",
    "                    \n",
    "                elif j == (n-1):\n",
    "                    #Right edge except corners\n",
    "                    sum_neighbors = array_in[i-1, j] + array_in[i+1, j] + array_in[i, j-1] + array_in[i-1, j-1] + array_in[i+1, j-1]\n",
    "                    avg = sum_neighbors/5\n",
    "                    \n",
    "                else:\n",
    "                    #Not on any edge or corner\n",
    "                    sum_neighbors = array_in[i, j+1] + array_in[i, j-1] + array_in[i-1, j] + array_in[i+1, j] + \\\n",
    "                                    array_in[i-1, j-1] + array_in[i-1, j+1] + array_in[i+1, j-1] + array_in[i+1, j+1]\n",
    "                    avg = sum_neighbors/8\n",
    "                    \n",
    "                    \n",
    "            array_out[i,j] = avg\n",
    "            #/for loop body\n",
    "        \n",
    "    return array_out\n",
    "                    \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I test the function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:33:18.794309Z",
     "iopub.status.busy": "2022-05-24T04:33:18.793462Z",
     "iopub.status.idle": "2022-05-24T04:33:18.801380Z",
     "shell.execute_reply": "2022-05-24T04:33:18.800512Z",
     "shell.execute_reply.started": "2022-05-24T04:33:18.794253Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test the function from above:\n",
    "import numpy as np\n",
    "arr = [[1,2,3], \n",
    "       [4,5,6],\n",
    "       [7,8,9]]\n",
    "arr = np.array(arr)\n",
    "arr_avgs = avg_neighbors(arr)\n",
    "print(arr_avgs)\n",
    "#Good: appears to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:02:04.413917Z",
     "iopub.status.busy": "2022-05-24T04:02:04.413558Z",
     "iopub.status.idle": "2022-05-24T04:02:04.425336Z",
     "shell.execute_reply": "2022-05-24T04:02:04.424260Z",
     "shell.execute_reply.started": "2022-05-24T04:02:04.413877Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's experiment to see if I know what's going on in the code cell 35 above:\n",
    "ls = ['Staring', 'at', 'the', 'tiny', 'planet', 'God', 'calculated', 'again']\n",
    "ls_iter = iter(ls)\n",
    "for i in range (len(ls)):\n",
    "    print(next(ls_iter))\n",
    "    \n",
    "#Ok, so far so good\n",
    "ls2 = [['There', 'was'], ['no', 'room'], ['for', 'a'], ['continuous', 'forest']]\n",
    "ls2_iter = iter(ls2)\n",
    "for i in range (len(ls2)):\n",
    "    first_word, second_word = next(ls2_iter)\n",
    "    print(first_word, second_word)\n",
    "#Yes, ok, working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:50:41.205783Z",
     "iopub.status.busy": "2022-05-24T04:50:41.205334Z",
     "iopub.status.idle": "2022-05-24T04:50:41.378222Z",
     "shell.execute_reply": "2022-05-24T04:50:41.377400Z",
     "shell.execute_reply.started": "2022-05-24T04:50:41.205737Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's try using the function above on the previous fire masks:\n",
    "#This code (is supposed to) find the first \"interesting\" previous fire mask and compute the avg number of neighboring \n",
    "#on-fire cells for each cell\n",
    "\n",
    "prev_fire_masks = inputs[:, :, :, 11] #observation number, pixel row, pixel col\n",
    "\n",
    "found_it_flag = 0 #will set to 1 once we've found our \"interesting\" fire mask\n",
    "img_num = 0\n",
    "while found_it_flag == 0:\n",
    "    fire_mask = np.array(prev_fire_masks[img_num, :, :])\n",
    "    if (np.all( (fire_mask == 0)) ): #if boring picture where there's no fire, toss it\n",
    "        img_num = img_num + 1\n",
    "    elif (np.all( np.invert(fire_mask == -1) )): #if NO data is missing data, cond. is TRUE --> you want this one\n",
    "        test_img = fire_mask\n",
    "        found_it_flag = 1\n",
    "    else:\n",
    "        img_num = img_num + 1\n",
    "\n",
    "np.set_printoptions(threshold=np.inf) #this just stops Jupyter from truncating the output\n",
    "print('fire mask:\\n', fire_mask, '\\n\\n')\n",
    "print('computed avg neighbor fire mask:\\n', avg_neighbors(fire_mask))\n",
    "#Note: don't freak out when you see the second matrix \"look bigger\" than the first. The dimensions are correct; the second matrix\n",
    "#is just visually larger because instead of 0s and 1s its entries are 3fs.\n",
    "#Tested with small grids (9x9) and appears to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next several cells, we eliminate observations where there is missing or uncertain data in the previous fire mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:50:46.784396Z",
     "iopub.status.busy": "2022-05-24T04:50:46.783427Z",
     "iopub.status.idle": "2022-05-24T04:52:36.148285Z",
     "shell.execute_reply": "2022-05-24T04:52:36.147050Z",
     "shell.execute_reply.started": "2022-05-24T04:50:46.784356Z"
    }
   },
   "outputs": [],
   "source": [
    "#Try to eliminate all observations where there are uncertain squares in the previous fire mask.\n",
    "prev_masks_array = np.array(inputs[:, :, :, 11])\n",
    "#print(prev_masks_array.shape) #100x32x32â€”good!\n",
    "\n",
    "#Build the array of certain data AND SAVE THE INDICES\n",
    "first_find_flag = 1\n",
    "count = 0\n",
    "indices = []\n",
    "\n",
    "for img_num in range(num_obs): \n",
    "    fire_mask = np.array(prev_fire_masks[img_num, :, :])\n",
    "    if (np.all( np.invert(fire_mask == -1) )): #If no missing data, condition is TRUE.\n",
    "        count += 1\n",
    "        indices.append(img_num)\n",
    "        if first_find_flag == 1: #If you need to start the array\n",
    "            certain_prev_fire_masks = fire_mask\n",
    "            first_find_flag = 0  #Remember to turn the flag off!\n",
    "        else:\n",
    "            certain_prev_fire_masks = np.dstack((certain_prev_fire_masks, fire_mask)) #d\n",
    "            \n",
    "\n",
    "#Test: You want the printouts from the following three lines to be consistent\n",
    "print(certain_prev_fire_masks.shape)\n",
    "print(count) \n",
    "print(len(indices))\n",
    "#Good, they are.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T04:56:02.908872Z",
     "iopub.status.busy": "2022-05-24T04:56:02.908455Z",
     "iopub.status.idle": "2022-05-24T05:16:34.389045Z",
     "shell.execute_reply": "2022-05-24T05:16:34.387819Z",
     "shell.execute_reply.started": "2022-05-24T04:56:02.908830Z"
    }
   },
   "outputs": [],
   "source": [
    "#Now that I've found the observations with \"pristine\" previous fire masks, I'd like to get those observations' features matrices\n",
    "full_input_array = np.array(inputs) #100x32x32x12\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    if i == 0:\n",
    "        certain_input_array = full_input_array[index,:,:,:]\n",
    "        print(certain_input_array.shape) #32x32x12\n",
    "    elif i == 1:\n",
    "        certain_input_array = np.concatenate((certain_input_array[..., np.newaxis], full_input_array[index,:,:,:, np.newaxis]), axis=3)\n",
    "    else:\n",
    "        certain_input_array = np.concatenate((certain_input_array, full_input_array[index,:,:,:, np.newaxis]), axis=3)\n",
    "        \n",
    "print(certain_input_array.shape)\n",
    "#SUCCESS! :) :) :) \n",
    "#certain_input_array now holds only the 77 observations with certain previous fire masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:16:52.795024Z",
     "iopub.status.busy": "2022-05-24T05:16:52.794640Z",
     "iopub.status.idle": "2022-05-24T05:24:28.624335Z",
     "shell.execute_reply": "2022-05-24T05:24:28.623309Z",
     "shell.execute_reply.started": "2022-05-24T05:16:52.794986Z"
    }
   },
   "outputs": [],
   "source": [
    "#Also want the labels at only these indices and the average neighbor value matrices\n",
    "full_labels = np.array(labels)\n",
    "#print(labels.shape) 100x32x32x1, as expected\n",
    "\n",
    "for i, index in enumerate(indices):\n",
    "    if i == 0:\n",
    "        #labels\n",
    "        certain_labels = full_labels[index,:,:,:] #32x32x1\n",
    "        \n",
    "        #avg neighbor values\n",
    "        surrounding_fire_scores = avg_neighbors(full_labels[index,:,:,:])\n",
    "        surrounding_fire_scores = surrounding_fire_scores[..., np.newaxis]\n",
    "        \n",
    "    else:\n",
    "        #labels\n",
    "        certain_labels = np.concatenate((certain_labels, full_labels[index,:,:,:]), axis=2)\n",
    "        \n",
    "        #avg neighbor values\n",
    "        avg_mat = avg_neighbors(full_labels[index,:,:,:])\n",
    "        surrounding_fire_scores = np.concatenate((surrounding_fire_scores, avg_mat[...,np.newaxis]), axis=2)\n",
    "\n",
    "#Printouts from following lines should agree...\n",
    "print(certain_labels.shape) #32x32x77 \n",
    "print(surrounding_fire_scores.shape) #32x32x77 \n",
    "#... and they do! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:24:46.911512Z",
     "iopub.status.busy": "2022-05-24T05:24:46.910868Z",
     "iopub.status.idle": "2022-05-24T05:24:46.920099Z",
     "shell.execute_reply": "2022-05-24T05:24:46.918877Z",
     "shell.execute_reply.started": "2022-05-24T05:24:46.911424Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to identify certain observations in the previous fire mask and return:\n",
    "#1) a list of their indices in the batch (need this to grab the right ones from the labels) and \n",
    "#2) the actual array of certain observations\n",
    "\n",
    "def elim_uncertain(prev_fire_mask_batch):\n",
    "    \n",
    "    prev_masks_array = np.array(prev_fire_mask_batch)\n",
    "    num_imgs, rows, cols = prev_masks_array.shape\n",
    "    \n",
    "    #Build the array of certain data AND SAVE THE INDICES\n",
    "    first_find_flag = 1\n",
    "    count = 0\n",
    "    indices = []\n",
    "\n",
    "    for img_num in range(num_imgs): \n",
    "        fire_mask = prev_fire_mask_batch[img_num, :, :] #grab the \"working fire mask\" off the pile\n",
    "        \n",
    "        if (np.all( np.invert(fire_mask == -1) )): #If no missing data, condition is TRUE.\n",
    "            count += 1\n",
    "            indices.append(img_num)\n",
    "            if first_find_flag == 1: #If you need to start the array\n",
    "                certain_prev_fire_masks_batch = fire_mask\n",
    "                first_find_flag = 0  #Remember to turn the flag off!\n",
    "            else:\n",
    "                certain_prev_fire_masks_batch = np.dstack((certain_prev_fire_masks_batch, fire_mask)) \n",
    "    \n",
    "    return certain_prev_fire_masks_batch, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:24:50.825214Z",
     "iopub.status.busy": "2022-05-24T05:24:50.824802Z",
     "iopub.status.idle": "2022-05-24T05:24:50.831982Z",
     "shell.execute_reply": "2022-05-24T05:24:50.831070Z",
     "shell.execute_reply.started": "2022-05-24T05:24:50.825168Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function to extract only the labels (i.e. current fire masks) from the certain observations\n",
    "def extract_certain_labels(certain_indices, og_labels):\n",
    "    \n",
    "    for i, index in enumerate(certain_indices):\n",
    "        if i == 0:\n",
    "            extracted_labels = og_labels[index,:,:,:] #the og_labels dimensions are batch_size by sidelength by sidelength by 1\n",
    "        else:\n",
    "            #labels\n",
    "            extracted_labels = np.concatenate((extracted_labels, og_labels[index,:,:,:]), axis=2)\n",
    "\n",
    "    return extracted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:24:53.610962Z",
     "iopub.status.busy": "2022-05-24T05:24:53.610419Z",
     "iopub.status.idle": "2022-05-24T05:24:53.616175Z",
     "shell.execute_reply": "2022-05-24T05:24:53.615548Z",
     "shell.execute_reply.started": "2022-05-24T05:24:53.610919Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create multi-D array of neighbor fire values\n",
    "def avg_neighbor_batch(batch_in):\n",
    "    rows, cols, batch_size = batch_in.shape #ordering of dimensions here meant to be compatible with elim_uncertain and extract_certain_labels\n",
    "    batch_out = np.zeros((rows, cols, batch_size))\n",
    "    for i in range(batch_size):\n",
    "        working_arr = batch_in[:,:,i]\n",
    "        avgd_arr = avg_neighbors(working_arr)\n",
    "        batch_out[:,:,i] =  avgd_arr\n",
    "    #/for loop\n",
    "    \n",
    "    return batch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:24:57.323793Z",
     "iopub.status.busy": "2022-05-24T05:24:57.323461Z",
     "iopub.status.idle": "2022-05-24T05:30:15.753003Z",
     "shell.execute_reply": "2022-05-24T05:30:15.751336Z",
     "shell.execute_reply.started": "2022-05-24T05:24:57.323755Z"
    }
   },
   "outputs": [],
   "source": [
    "#Test functions above:\n",
    "import numpy as np\n",
    "full_prev_fire_masks = np.array(inputs[:,:,:,11]) #just grabs the previous fire masks from the full observation multi-D array\n",
    "full_curr_fire_masks = np.array(labels) #100x32x32x1\n",
    "\n",
    "certain_prev_masks, certain_indices = elim_uncertain(full_prev_fire_masks) #32x32x86 (generally: sidelength^2 x #certain obs.)\n",
    "certain_labels = extract_certain_labels(certain_indices, full_curr_fire_masks) #32x32x86 (generally: sidelength^2 x #certain obs.)\n",
    "avg_neighbors_feat = avg_neighbor_batch(certain_prev_masks) #32x32x86 (generally: sidelength^2 x #certain obs.)\n",
    "\n",
    "#Success! Note that new format for certain_labels and certain_prev_masks is rows, cols, index in (new, certain-only) pile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:59:15.155486Z",
     "iopub.status.busy": "2022-05-24T05:59:15.154926Z",
     "iopub.status.idle": "2022-05-24T05:59:15.162763Z",
     "shell.execute_reply": "2022-05-24T05:59:15.161885Z",
     "shell.execute_reply.started": "2022-05-24T05:59:15.155418Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = certain_input_array\n",
    "Y_train = certain_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzCgCoxtgP-f"
   },
   "source": [
    "# Plotting function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data!\n",
    "\n",
    "First we define the names for each of our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:31:40.936322Z",
     "iopub.status.busy": "2022-05-24T05:31:40.935301Z",
     "iopub.status.idle": "2022-05-24T05:31:40.941355Z",
     "shell.execute_reply": "2022-05-24T05:31:40.940497Z",
     "shell.execute_reply.started": "2022-05-24T05:31:40.936264Z"
    },
    "id": "bnG0_l_ChjUt"
   },
   "outputs": [],
   "source": [
    "TITLES = [\n",
    "  'Elevation',\n",
    "  'Wind\\ndirection',\n",
    "  'Wind\\nvelocity',\n",
    "  'Min\\ntemp',\n",
    "  'Max\\ntemp',\n",
    "  'Humidity',\n",
    "  'Precip',\n",
    "  'Drought',\n",
    "  'Vegetation',\n",
    "  'Population\\ndensity',\n",
    "  'Energy\\nrelease\\ncomponent',\n",
    "  'Previous\\nfire\\nmask',\n",
    "  'Fire\\nmask'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some helper variables for the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:31:45.579041Z",
     "iopub.status.busy": "2022-05-24T05:31:45.578730Z",
     "iopub.status.idle": "2022-05-24T05:31:45.585791Z",
     "shell.execute_reply": "2022-05-24T05:31:45.584653Z",
     "shell.execute_reply.started": "2022-05-24T05:31:45.579008Z"
    },
    "id": "G0E6lWR9beD0"
   },
   "outputs": [],
   "source": [
    "# Number of rows of data samples to plot\n",
    "n_rows = 5 \n",
    "# Number of data variables\n",
    "n_features = inputs.shape[3]\n",
    "# Variables for controllong the color map for the fire masks\n",
    "CMAP = colors.ListedColormap(['black', 'silver', 'orangered'])\n",
    "BOUNDS = [-1, -0.1, 0.001, 1]\n",
    "NORM = colors.BoundaryNorm(BOUNDS, CMAP.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:31:48.229957Z",
     "iopub.status.busy": "2022-05-24T05:31:48.229611Z",
     "iopub.status.idle": "2022-05-24T05:31:52.065221Z",
     "shell.execute_reply": "2022-05-24T05:31:52.064525Z",
     "shell.execute_reply.started": "2022-05-24T05:31:48.229919Z"
    },
    "id": "sPtKQzQv71J_",
    "outputId": "6694ad6e-0044-4fbc-b184-f615ac14a885"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,6.5))\n",
    "\n",
    "for i in range(n_rows):\n",
    "  for j in range(n_features + 1):\n",
    "    plt.subplot(n_rows, n_features + 1, i * (n_features + 1) + j + 1)\n",
    "    if i == 0:\n",
    "      plt.title(TITLES[j], fontsize=13)\n",
    "    if j < n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, j], cmap='viridis')\n",
    "    if j == n_features - 1:\n",
    "      plt.imshow(inputs[i, :, :, -1], cmap=CMAP, norm=NORM)\n",
    "    if j == n_features:\n",
    "      plt.imshow(labels[i, :, :, 0], cmap=CMAP, norm=NORM) \n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T05:35:19.261649Z",
     "iopub.status.busy": "2022-05-24T05:35:19.261130Z",
     "iopub.status.idle": "2022-05-24T05:35:19.268142Z",
     "shell.execute_reply": "2022-05-24T05:35:19.266970Z",
     "shell.execute_reply.started": "2022-05-24T05:35:19.261611Z"
    }
   },
   "outputs": [],
   "source": [
    "certain_prev_fire_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-13T17:37:52.731108Z",
     "iopub.status.busy": "2022-05-13T17:37:52.730659Z",
     "iopub.status.idle": "2022-05-13T17:37:52.736571Z",
     "shell.execute_reply": "2022-05-13T17:37:52.735337Z",
     "shell.execute_reply.started": "2022-05-13T17:37:52.731075Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T21:28:31.924773Z",
     "iopub.status.busy": "2022-05-21T21:28:31.924213Z",
     "iopub.status.idle": "2022-05-21T21:28:31.991332Z",
     "shell.execute_reply": "2022-05-21T21:28:31.989783Z",
     "shell.execute_reply.started": "2022-05-21T21:28:31.924718Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
